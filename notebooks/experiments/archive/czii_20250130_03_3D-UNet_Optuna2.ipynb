{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pkgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This is training notebook only. Inference ain't included in . \n",
    "Anybody who wants to use this notebook for inference purposes is most welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import zarr, copick\n",
    "from tqdm import tqdm\n",
    "import napari\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from copick_utils.segmentation import segmentation_from_picks\n",
    "import copick_utils.io.writers as write\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "import concurrent.futures\n",
    "import optuna, optunahub\n",
    "import json\n",
    "import copy\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.cache_size_limit = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/'\n",
    "output_path = path + 'output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a copick project\n",
    "\n",
    "config_blob = \"\"\"{\n",
    "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
    "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
    "    \"version\": \"1.0.0\",\n",
    "\n",
    "    \"pickable_objects\": [\n",
    "        {\n",
    "            \"name\": \"apo-ferritin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"4V1W\",\n",
    "            \"label\": 1,\n",
    "            \"color\": [  0, 117, 220, 128],\n",
    "            \"radius\": 60,\n",
    "            \"map_threshold\": 0.0418\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"beta-galactosidase\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6X1Q\",\n",
    "            \"label\": 2,\n",
    "            \"color\": [ 76,   0,  92, 128],\n",
    "            \"radius\": 90,\n",
    "            \"map_threshold\": 0.0578\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ribosome\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6EK0\",\n",
    "            \"label\": 3,\n",
    "            \"color\": [  0,  92,  49, 128],\n",
    "            \"radius\": 150,\n",
    "            \"map_threshold\": 0.0374\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"thyroglobulin\",\n",
    "            \"is_particle\": true,\n",
    "            \"pdb_id\": \"6SCJ\",\n",
    "            \"label\": 4,\n",
    "            \"color\": [ 43, 206,  72, 128],\n",
    "            \"radius\": 130,\n",
    "            \"map_threshold\": 0.0278\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"virus-like-particle\",\n",
    "            \"is_particle\": true,\n",
    "            \"label\": 5,\n",
    "            \"color\": [255, 204, 153, 128],\n",
    "            \"radius\": 135,\n",
    "            \"map_threshold\": 0.201\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    \"overlay_root\": \"/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/output/overlay\",\n",
    "\n",
    "    \"overlay_fs_args\": {\n",
    "        \"auto_mkdir\": true\n",
    "    },\n",
    "\n",
    "    \"static_root\": \"/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/train/static\"\n",
    "}\"\"\"\n",
    "\n",
    "copick_config_path = path + \"output/copick.config\"\n",
    "output_overlay = path + \"output/overlay\"\n",
    "\n",
    "with open(copick_config_path, \"w\") as f:\n",
    "    f.write(config_blob)\n",
    "    \n",
    "# Update the overlay\n",
    "# Define source and destination directories\n",
    "source_dir = path + 'train/overlay'\n",
    "destination_dir = path + 'output/overlay'\n",
    "\n",
    "# Walk through the source directory\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    # Create corresponding subdirectories in the destination\n",
    "    relative_path = os.path.relpath(root, source_dir)\n",
    "    target_dir = os.path.join(destination_dir, relative_path)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy and rename each file\n",
    "    for file in files:\n",
    "        if file.startswith(\"curation_0_\"):\n",
    "            new_filename = file\n",
    "        else:\n",
    "            new_filename = f\"curation_0_{file}\"\n",
    "            \n",
    "        \n",
    "        # Define full paths for the source and destination files\n",
    "        source_file = os.path.join(root, file)\n",
    "        destination_file = os.path.join(target_dir, new_filename)\n",
    "        \n",
    "        # Copy the file with the new name\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f\"Copied {source_file} to {destination_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset\n",
    "## 1. Get copick root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = copick.from_file(copick_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 10\n",
    "#tomo_type = \"denoised\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate multi-class segmentation masks from picks, and saved them to the copick overlay directory (one-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just do this once\n",
    "generate_masks = True\n",
    "\n",
    "if generate_masks:\n",
    "    target_objects = defaultdict(dict)\n",
    "    for object in root.pickable_objects:\n",
    "        if object.is_particle:\n",
    "            target_objects[object.name]['label'] = object.label\n",
    "            target_objects[object.name]['radius'] = object.radius\n",
    "\n",
    "\n",
    "    for run in tqdm(root.runs):\n",
    "        tomo = run.get_voxel_spacing(voxel_size)\n",
    "        for tomogram in tomo.tomograms:\n",
    "            tomo_type = tomogram.tomo_type\n",
    "            image = tomogram.numpy()\n",
    "            target = np.zeros(image.shape, dtype=np.uint8)\n",
    "            for pickable_object in root.pickable_objects:\n",
    "                pick = run.get_picks(object_name=pickable_object.name, user_id='curation')\n",
    "                if len(pick):\n",
    "                    target = segmentation_from_picks.from_picks(pick[0],\n",
    "                                                                target,\n",
    "                                                                target_objects[pickable_object.name]['radius'],# * 0.5,\n",
    "                                                                target_objects[pickable_object.name]['label']\n",
    "                                                               )\n",
    "            write.segmentation(run, target, copick_user_name, name=copick_segmentation_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get tomograms and their segmentaion masks (from picks) arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_experiment_folders_path = '/media/max1024/Extreme SSD1/Kaggle/czii-cryo-et-object-identification/' + 'train/overlay/ExperimentRuns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids = {\n",
    "    'apo-ferritin': 1,\n",
    "    'beta-galactosidase': 2,\n",
    "    'ribosome': 3,\n",
    "    'thyroglobulin': 4,\n",
    "    'virus-like-particle': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_radius = {\n",
    "    'apo-ferritin': 60,\n",
    "    'beta-galactosidase': 90,\n",
    "    'ribosome': 150,\n",
    "    'thyroglobulin': 130,\n",
    "    'virus-like-particle': 135,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_df(experiment):\n",
    "    labels_dict = {}\n",
    "    \n",
    "    particle_types_dict = {}\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/apo-ferritin.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['apo-ferritin'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/beta-galactosidase.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['beta-galactosidase'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/ribosome.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['ribosome'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/thyroglobulin.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['thyroglobulin'] = loaded_json\n",
    "    \n",
    "    with open(f'{train_label_experiment_folders_path}{experiment}/Picks/virus-like-particle.json') as f:\n",
    "        loaded_json = json.loads(f.read())\n",
    "    particle_types_dict['virus-like-particle'] = loaded_json\n",
    "    \n",
    "    labels_dict[experiment] = particle_types_dict\n",
    "\n",
    "    experiment_list = []\n",
    "    particle_type_list = []\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    r_list = []\n",
    "    class_id_list = []\n",
    "    #print(experiment)\n",
    "    #print(len(labels_dict[experiment]['apo-ferritin']['points']))\n",
    "    #print(type(labels_dict[experiment]['apo-ferritin']['points']))\n",
    "    #print(labels_dict[experiment]['apo-ferritin']['points'][0])\n",
    "\n",
    "    for key in labels_dict[experiment].keys():\n",
    "        #print(labels_dict[experiment][key])\n",
    "        #print(labels_dict[experiment][key]['pickable_object_name'])\n",
    "        for i in range(len(labels_dict[experiment][key]['points'])):\n",
    "            experiment_list.append(labels_dict[experiment][key]['run_name'])\n",
    "            particle_type_list.append(labels_dict[experiment][key]['pickable_object_name'])\n",
    "            x_list.append(labels_dict[experiment][key]['points'][i]['location']['x']/10.012444537618887)\n",
    "            y_list.append(labels_dict[experiment][key]['points'][i]['location']['y']/10.012444196428572)\n",
    "            z_list.append(labels_dict[experiment][key]['points'][i]['location']['z']/10.012444196428572)\n",
    "            r_list.append(particle_radius[key]/10)\n",
    "            class_id_list.append(class_ids[key])\n",
    "\n",
    "    labels_df = pd.DataFrame({'experiment':experiment_list, 'particle_type':particle_type_list, 'x':x_list, 'y':y_list, 'z':z_list, 'radius':r_list, 'label':class_id_list})\n",
    "    \n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts = []\n",
    "for run in tqdm(root.runs):\n",
    "    tomo = run.get_voxel_spacing(voxel_size)#.get_tomograms(tomo_type)[0].numpy()\n",
    "    labels_df = create_labels_df(run.name)\n",
    "    for tomogram in tomo.tomograms:\n",
    "        tomo_type = tomogram.tomo_type\n",
    "        image = tomogram.numpy()\n",
    "        segmentation = run.get_segmentations(name=copick_segmentation_name, user_id=copick_user_name, voxel_size=voxel_size, is_multilabel=True)[0].numpy()\n",
    "        data_dicts.append({\"tomo_type\":tomo_type, \"image\": image, \"label\": segmentation, \"label_df\": labels_df})\n",
    "    \n",
    "print(np.unique(data_dicts[0]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[0]['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[0]['label_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the tomogram and painted segmentation from ground-truth picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Tomogram')\n",
    "plt.imshow(data_dicts[0]['image'][100],cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Painted Segmentation from Picks')\n",
    "plt.imshow(data_dicts[0]['label'][100], cmap='viridis')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_data_path = '../../czii_downloaded_data/simulated_training_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulated_experiments_list = [f for f in os.listdir(simulated_data_path) if 'TS_' in f]\n",
    "simulated_experiments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_info = {\n",
    "    \"apo-ferritin\": {\"label\": 1, \"radius\": 60},\n",
    "    \"beta-galactosidase\": {\"label\": 2, \"radius\": 90},\n",
    "    \"ribosome\": {\"label\": 3, \"radius\": 150},\n",
    "    \"thyroglobulin\": {\"label\": 4, \"radius\": 130},\n",
    "    \"virus-like-particle\": {\"label\": 5, \"radius\": 135}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(image, locations_df):\n",
    "    shape = image.shape\n",
    "    volume = np.zeros(shape, dtype=np.uint8)\n",
    "\n",
    "    for i in range(locations_df.shape[0]):\n",
    "        center = (locations_df.loc[i, 'z'], locations_df.loc[i, 'y'], locations_df.loc[i, 'x'])\n",
    "        radius = locations_df.loc[i, 'radius']\n",
    "        intensity = locations_df.loc[i, 'label']\n",
    "\n",
    "        z, y, x = np.ogrid[:shape[0], :shape[1], :shape[2]]\n",
    "        \n",
    "        mask = (x - center[2])**2 + (y - center[1])**2 + (z - center[0])**2 <= radius**2\n",
    "\n",
    "        volume[mask] = intensity\n",
    "\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_name_dict = {\n",
    "    'ferritin':'apo-ferritin',\n",
    "    'galacto':'beta-galactosidase',\n",
    "    'ribosome':'ribosome',\n",
    "    'thyro':'thyroglobulin',\n",
    "    'vlp':'virus-like-particle'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(experiment):\n",
    "    \"\"\"Process a single experiment in parallel\"\"\"\n",
    "    # Load tomogram data\n",
    "    tomogram_path = f'{simulated_data_path}{experiment}/Reconstructions/VoxelSpacing10.000/Tomograms/100/{experiment}.zarr'\n",
    "    tomogram = zarr.open(tomogram_path, mode='r')[0].__array__()\n",
    "    \n",
    "    location_dfs_list = []\n",
    "    annotations_base = f'{simulated_data_path}{experiment}/Reconstructions/VoxelSpacing10.000/Annotations/'\n",
    "    \n",
    "    # Process location files\n",
    "    for location_folder in os.listdir(annotations_base):\n",
    "        location_dir = os.path.join(annotations_base, location_folder)\n",
    "        for location_file in os.listdir(location_dir):\n",
    "            if 'orientedpoint.ndjson' not in location_file:\n",
    "                continue\n",
    "                \n",
    "            # Determine particle type\n",
    "            particle_type = None\n",
    "            for key in particle_name_dict.keys():\n",
    "                if key in location_file:\n",
    "                    particle_type = particle_name_dict[key]\n",
    "                    break\n",
    "            if not particle_type:\n",
    "                continue\n",
    "                \n",
    "            # Process NDJSON file\n",
    "            label = particle_info[particle_type]['label']\n",
    "            radius = particle_info[particle_type]['radius']\n",
    "            file_path = os.path.join(location_dir, location_file)\n",
    "            \n",
    "            location_df = pd.read_json(file_path, lines=True)\n",
    "            location_df['x'] = location_df['location'].map(lambda x: x['x'])\n",
    "            location_df['y'] = location_df['location'].map(lambda x: x['y'])\n",
    "            location_df['z'] = location_df['location'].map(lambda x: x['z'])\n",
    "            location_df['label'] = label\n",
    "            location_df['radius'] = radius / 10# * 0.5\n",
    "            location_df['particle_type'] = particle_type\n",
    "            \n",
    "            location_dfs_list.append(location_df)\n",
    "    \n",
    "    # Create mask and return result\n",
    "    if location_dfs_list:\n",
    "        all_particle_locations_df = pd.concat(location_dfs_list, ignore_index=True)\n",
    "        mask_image = create_masks(tomogram, all_particle_locations_df)\n",
    "        all_particle_locations_df['experiment'] = experiment\n",
    "        label_df = all_particle_locations_df[['experiment', 'particle_type', 'x', 'y', 'z', 'radius', 'label']]\n",
    "        return {\"tomo_type\": 'Unknown', \"image\": tomogram, \"label\": mask_image, \"label_df\": label_df}\n",
    "    return None\n",
    "\n",
    "def append_simulation_data():\n",
    "    global data_dicts\n",
    "    max_workers = os.cpu_count() // 2  # Use half the available cores to prevent memory issues\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all experiments for processing\n",
    "        futures = [executor.submit(process_experiment, exp) for exp in simulated_experiments_list]\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                         total=len(simulated_experiments_list),\n",
    "                         desc=\"Processing experiments\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                data_dicts.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "append_simulation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_image = data_dicts[-1]['image']\n",
    "temp_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_label = data_dicts[-1]['label']\n",
    "temp_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp_image[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp_label[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[-1]['label_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss, FocalLoss, TverskyLoss\n",
    "from monai.metrics import DiceMetric, ConfusionMatrixMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "numbers = list(range(len(data_dicts)))\n",
    "random_numbers = random.sample(numbers, int(len(data_dicts)/5))\n",
    "\n",
    "print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files = []\n",
    "train_files = []\n",
    "for i in range(len(data_dicts)):\n",
    "    if i in random_numbers:\n",
    "        val_files.append(data_dicts[i])\n",
    "    else:\n",
    "        train_files.append(data_dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(train_files)}\")\n",
    "print(f\"Number of validation samples: {len(val_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6 # 5 particles + 1 background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_copy = copy.deepcopy(train_files)\n",
    "val_files_copy = copy.deepcopy(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_files)):\n",
    "    del train_files[i]['tomo_type'], train_files[i]['label_df']\n",
    "\n",
    "for i in range(len(val_files)):\n",
    "    del val_files[i]['tomo_type'], val_files[i]['label_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_num_samples = 16\n",
    "train_batch_size = 1\n",
    "val_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-random transforms to be cached\n",
    "non_random_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\")\n",
    "])\n",
    "\n",
    "# Random transforms to be applied during training\n",
    "random_transforms = Compose([\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[96, 96, 96],\n",
    "        num_classes=num_classes,\n",
    "        num_samples=my_num_samples,\n",
    "        allow_missing_keys=True\n",
    "    ),\n",
    "    RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "    RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),    \n",
    "])\n",
    "\n",
    "# Create the cached dataset with non-random transforms\n",
    "train_ds = CacheDataset(data=train_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "# Wrap the cached dataset to apply random transforms during iteration\n",
    "train_ds = Dataset(data=train_ds, transform=random_transforms)\n",
    "\n",
    "# DataLoader remains the same\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Validation transforms\n",
    "val_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\", \"label\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    RandCropByLabelClassesd(\n",
    "        keys=[\"image\", \"label\"],\n",
    "        label_key=\"label\",\n",
    "        spatial_size=[96, 96, 96],\n",
    "        num_classes=num_classes,\n",
    "        num_samples=my_num_samples,  # Use 1 to get a single, consistent crop per image\n",
    "        allow_missing_keys=True\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Create validation dataset\n",
    "val_ds = CacheDataset(data=val_files, transform=non_random_transforms, cache_rate=1.0)\n",
    "\n",
    "# Wrap the cached dataset to apply random transforms during iteration\n",
    "val_ds = Dataset(data=val_ds, transform=random_transforms)\n",
    "\n",
    "# Create validation DataLoader\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=val_batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    "    shuffle=False,  # Ensure the data order remains consistent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Derived from:\n",
    "https://github.com/cellcanvas/album-catalog/blob/main/solutions/copick/compare-picks/solution.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_metrics(reference_points, reference_radius, candidate_points):\n",
    "    num_reference_particles = len(reference_points)\n",
    "    num_candidate_particles = len(candidate_points)\n",
    "\n",
    "    if len(reference_points) == 0:\n",
    "        return 0, num_candidate_particles, 0\n",
    "\n",
    "    if len(candidate_points) == 0:\n",
    "        return 0, 0, num_reference_particles\n",
    "\n",
    "    ref_tree = KDTree(reference_points)\n",
    "    candidate_tree = KDTree(candidate_points)\n",
    "    raw_matches = candidate_tree.query_ball_tree(ref_tree, r=reference_radius)\n",
    "    matches_within_threshold = []\n",
    "    for match in raw_matches:\n",
    "        matches_within_threshold.extend(match)\n",
    "    # Prevent submitting multiple matches per particle.\n",
    "    # This won't be be strictly correct in the (extremely rare) case where true particles\n",
    "    # are very close to each other.\n",
    "    matches_within_threshold = set(matches_within_threshold)\n",
    "    tp = int(len(matches_within_threshold))\n",
    "    fp = int(num_candidate_particles - tp)\n",
    "    fn = int(num_reference_particles - tp)\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        row_id_column_name: str,\n",
    "        distance_multiplier: float,\n",
    "        beta: int) -> float:\n",
    "    '''\n",
    "    F_beta\n",
    "      - a true positive occurs when\n",
    "         - (a) the predicted location is within a threshold of the particle radius, and\n",
    "         - (b) the correct `particle_type` is specified\n",
    "      - raw results (TP, FP, FN) are aggregated across all experiments for each particle type\n",
    "      - f_beta is calculated for each particle type\n",
    "      - individual f_beta scores are weighted by particle type for final score\n",
    "    '''\n",
    "\n",
    "    particle_radius = {\n",
    "        'apo-ferritin': 60,\n",
    "        'beta-amylase': 65,\n",
    "        'beta-galactosidase': 90,\n",
    "        'ribosome': 150,\n",
    "        'thyroglobulin': 130,\n",
    "        'virus-like-particle': 135,\n",
    "    }\n",
    "\n",
    "    weights = {\n",
    "        'apo-ferritin': 1,\n",
    "        'beta-amylase': 0,\n",
    "        'beta-galactosidase': 2,\n",
    "        'ribosome': 1,\n",
    "        'thyroglobulin': 2,\n",
    "        'virus-like-particle': 1,\n",
    "    }\n",
    "\n",
    "    particle_radius = {k: v * distance_multiplier for k, v in particle_radius.items()}\n",
    "\n",
    "    # Filter submission to only contain experiments found in the solution split\n",
    "    split_experiments = set(solution['experiment'].unique())\n",
    "    submission = submission.loc[submission['experiment'].isin(split_experiments)]\n",
    "\n",
    "    # Only allow known particle types\n",
    "    if not set(submission['particle_type'].unique()).issubset(set(weights.keys())):\n",
    "        raise ParticipantVisibleError('Unrecognized `particle_type`.')\n",
    "\n",
    "    assert solution.duplicated(subset=['experiment', 'x', 'y', 'z']).sum() == 0\n",
    "    assert particle_radius.keys() == weights.keys()\n",
    "\n",
    "    results = {}\n",
    "    for particle_type in solution['particle_type'].unique():\n",
    "        results[particle_type] = {\n",
    "            'total_tp': 0,\n",
    "            'total_fp': 0,\n",
    "            'total_fn': 0,\n",
    "        }\n",
    "\n",
    "    for experiment in split_experiments:\n",
    "        for particle_type in solution['particle_type'].unique():\n",
    "            reference_radius = particle_radius[particle_type]\n",
    "            select = (solution['experiment'] == experiment) & (solution['particle_type'] == particle_type)\n",
    "            reference_points = solution.loc[select, ['x', 'y', 'z']].values\n",
    "\n",
    "            select = (submission['experiment'] == experiment) & (submission['particle_type'] == particle_type)\n",
    "            candidate_points = submission.loc[select, ['x', 'y', 'z']].values\n",
    "\n",
    "            if len(reference_points) == 0:\n",
    "                reference_points = np.array([])\n",
    "                reference_radius = 1\n",
    "\n",
    "            if len(candidate_points) == 0:\n",
    "                candidate_points = np.array([])\n",
    "\n",
    "            tp, fp, fn = compute_metrics(reference_points, reference_radius, candidate_points)\n",
    "\n",
    "            results[particle_type]['total_tp'] += tp\n",
    "            results[particle_type]['total_fp'] += fp\n",
    "            results[particle_type]['total_fn'] += fn\n",
    "\n",
    "    aggregate_fbeta = 0.0\n",
    "    for particle_type, totals in results.items():\n",
    "        tp = totals['total_tp']\n",
    "        fp = totals['total_fp']\n",
    "        fn = totals['total_fn']\n",
    "\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        aggregate_fbeta += fbeta * weights.get(particle_type, 1.0)\n",
    "\n",
    "    if weights:\n",
    "        aggregate_fbeta = aggregate_fbeta / sum(weights.values())\n",
    "    else:\n",
    "        aggregate_fbeta = aggregate_fbeta / len(results)\n",
    "    return aggregate_fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBetaLoss(nn.Module):\n",
    "    def __init__(self, beta=4, eps=1e-6, class_weights=None, enzyme_classes=5):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.enzyme_classes = enzyme_classes\n",
    "        \n",
    "        self.class_weights = class_weights or {\n",
    "            'apo-ferritin': 1,\n",
    "            'beta-galactosidase': 2,\n",
    "            'ribosome': 1,\n",
    "            'thyroglobulin': 2,\n",
    "            'virus-like-particle': 1,\n",
    "        }\n",
    "        self.weight_tensor = torch.tensor(\n",
    "            [self.class_weights[k] for k in self.class_weights.keys()],\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Convert y_true to proper format\n",
    "        y_true = y_true.squeeze(1).long()  # Remove channel dim & ensure integer type\n",
    "        \n",
    "        y_pred_probs = F.softmax(y_pred, dim=1)\n",
    "        y_pred_enzymes = y_pred_probs[:, 1:self.enzyme_classes+1, ...]\n",
    "        \n",
    "        # One-hot encoding with proper type handling\n",
    "        y_true_onehot = F.one_hot(y_true, num_classes=6).permute(0, 4, 1, 2, 3).float()\n",
    "        y_true_enzymes = y_true_onehot[:, 1:self.enzyme_classes+1, ...]\n",
    "\n",
    "        tp = (y_pred_enzymes * y_true_enzymes).sum(dim=(2, 3, 4))\n",
    "        fp = (y_pred_enzymes * (1 - y_true_enzymes)).sum(dim=(2, 3, 4))\n",
    "        fn = ((1 - y_pred_enzymes) * y_true_enzymes).sum(dim=(2, 3, 4))\n",
    "\n",
    "        precision = tp / (tp + fp + self.eps)\n",
    "        recall = tp / (tp + fn + self.eps)\n",
    "        beta2 = self.beta ** 2\n",
    "        f_beta = (1 + beta2) * (precision * recall) / (beta2 * precision + recall + self.eps)\n",
    "\n",
    "        weight_tensor = self.weight_tensor.to(y_pred.device)\n",
    "        weighted_f_beta = f_beta * weight_tensor\n",
    "        aggregate_f_beta = weighted_f_beta.sum(dim=1) / weight_tensor.sum()\n",
    "        \n",
    "        return 1 - aggregate_f_beta.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbeta_loss_function = FBetaLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleTverskyCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, particle_weights, alpha=16/17, beta=1/17, ce_weight=1.0, tversky_weight=1.0, smooth=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            particle_weights (torch.Tensor): Weights for 5 particles + background [0.0, 1.0, 2.0, 1.0, 2.0, 1.0]\n",
    "            alpha: Tversky FP weight (prioritizes recall for beta=4)\n",
    "            beta: Tversky FN weight\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.tversky_weight = tversky_weight\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Cross-Entropy with particle weights (background weight=0)\n",
    "        self.ce = nn.CrossEntropyLoss(weight=particle_weights)\n",
    "        self.register_buffer('class_weights', particle_weights)  # [6] tensor\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Ensure targets have the correct shape and dtype\n",
    "        ce_loss = self.ce(inputs, targets.squeeze(1).long())\n",
    "    \n",
    "        # Tversky Loss (3D-compatible)\n",
    "        num_classes = inputs.shape[1]\n",
    "        probs = F.softmax(inputs, dim=1)\n",
    "        targets_onehot = F.one_hot(targets.squeeze(1).long(), num_classes).permute(0, 4, 1, 2, 3).float()  # BCDHW\n",
    "    \n",
    "        # Flatten spatial dimensions (3D)\n",
    "        probs_flat = probs.view(probs.size(0), num_classes, -1)  # [B,6,D*H*W]\n",
    "        targets_flat = targets_onehot.view(targets_onehot.size(0), num_classes, -1)\n",
    "    \n",
    "        # Calculate TP/FP/FN (broadcasted across classes)\n",
    "        tp = (probs_flat * targets_flat).sum(dim=2)  # [B,6]\n",
    "        fp = (probs_flat * (1 - targets_flat)).sum(dim=2)\n",
    "        fn = ((1 - probs_flat) * targets_flat).sum(dim=2)\n",
    "    \n",
    "        # Tversky index per class\n",
    "        tversky = (tp + self.smooth) / (tp + self.alpha*fp + self.beta*fn + self.smooth)  # [B,6]\n",
    "        tversky_loss = 1 - tversky\n",
    "    \n",
    "        # Apply class weights\n",
    "        weighted_tversky = tversky_loss * self.class_weights  # [B,6]\n",
    "        tversky_loss = weighted_tversky.mean()\n",
    "    \n",
    "        # Combined loss\n",
    "        return self.ce_weight * ce_loss + self.tversky_weight * tversky_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class order: [background, apo-ferritin, beta-galactosidase, ribosome, thyroglobulin, virus-like-particle]\n",
    "particle_weights = torch.tensor([\n",
    "    0.0,  # Background (weight=0)\n",
    "    1.0,  # apo-ferritin\n",
    "    2.0,  # beta-galactosidase (higher weight)\n",
    "    1.0,  # ribosome\n",
    "    2.0,  # thyroglobulin (higher weight)\n",
    "    1.0   # virus-like-particle\n",
    "], device='cuda')\n",
    "\n",
    "wtversky_ce_loss_function = ParticleTverskyCrossEntropyLoss(\n",
    "    particle_weights=particle_weights,\n",
    "    ce_weight=1.0,  # Balance with Tversky\n",
    "    tversky_weight=1.0  # Emphasize recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = DiceLoss(include_background=True, to_onehot_y=True, softmax=True)  # softmax=True for multiclass\n",
    "tversky_loss_function = TverskyLoss(include_background=False, to_onehot_y=True, softmax=True, alpha=16/17, beta=1/17)  # softmax=True for multiclass\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", ignore_empty=True)  # must use onehot for multiclass\n",
    "recall_metric = ConfusionMatrixMetric(include_background=False, metric_name=\"recall\", reduction=\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pred = AsDiscrete(argmax=True, to_onehot=num_classes)\n",
    "post_label = AsDiscrete(to_onehot=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "import cc3d\n",
    "\n",
    "\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "    \n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "    \n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates\n",
    "\n",
    "    \n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "    \n",
    "    patch_size = patches[0].shape[0]\n",
    "    \n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch\n",
    "        \n",
    "    \n",
    "    return reconstructed\n",
    "\n",
    "    \n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "        \n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "    \n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "    \n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "    \n",
    "    return positions\n",
    "    \n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              #2: \"beta-amylase\",\n",
    "              2: \"beta-galactosidase\", \n",
    "              3: \"ribosome\", \n",
    "              4: \"thyroglobulin\", \n",
    "              5: \"virus-like-particle\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_prediction_tta(model, input_tensor, threshold=0.05):\n",
    "    probs_list = []\n",
    "    \n",
    "    data_copy0 = input_tensor.clone()\n",
    "    data_copy0 = torch.flip(data_copy0, dims=[2])\n",
    "    data_copy1 = input_tensor.clone()\n",
    "    data_copy1 = torch.flip(data_copy1, dims=[3])\n",
    "    data_copy2 = input_tensor.clone()\n",
    "    data_copy2 = torch.flip(data_copy2, dims=[4])\n",
    "    data_copy3 = input_tensor.clone()\n",
    "    data_copy3 = data_copy3.rot90(1, dims=[3, 4])\n",
    "    \n",
    "    model_output0 = model(input_tensor)\n",
    "    model_output1 = model(data_copy0)\n",
    "    model_output1 = torch.flip(model_output1, dims=[2])\n",
    "    model_output2 = model(data_copy1)\n",
    "    model_output2 = torch.flip(model_output2, dims=[3])\n",
    "    model_output3 = model(data_copy2)\n",
    "    model_output3 = torch.flip(model_output3, dims=[4])\n",
    "    \n",
    "    probs0 = torch.softmax(model_output0[0], dim=0)\n",
    "    probs1 = torch.softmax(model_output1[0], dim=0)\n",
    "    probs2 = torch.softmax(model_output2[0], dim=0)\n",
    "    probs3 = torch.softmax(model_output3[0], dim=0)\n",
    "    \n",
    "    probs_list.append(probs0)\n",
    "    probs_list.append(probs1)\n",
    "    probs_list.append(probs2)\n",
    "    probs_list.append(probs3)\n",
    "    \n",
    "    avg_probs = torch.mean(torch.stack(probs_list), dim=0)\n",
    "    thresh_probs = avg_probs > threshold\n",
    "    _, max_classes = thresh_probs.max(dim=0)\n",
    "    \n",
    "    return max_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_dict = {\n",
    "    'fbeta': fbeta_loss_function,\n",
    "    'wtversky_ce': wtversky_ce_loss_function,\n",
    "    'tversky': tversky_loss_function\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    learning_rate = trial.suggest_float('learning_rate', 3e-4, 3e-3)\n",
    "    third_channel = trial.suggest_int('third_channel', 80, 128)\n",
    "    fourth_channel = trial.suggest_int('fourth_channel', 80, 128)\n",
    "    last_stride = trial.suggest_int('last_stride', 1, 2)\n",
    "    #batch_size = trial.suggest_int('batch_size', 1, 2)\n",
    "    num_res_units = trial.suggest_int('num_res_units', 1, 4)\n",
    "    loss_function_name = trial.suggest_categorical('loss_function', ['fbeta', 'wtversky_ce', 'tversky'])\n",
    "    loss_function = loss_function_dict[loss_function_name]\n",
    "\n",
    "    # Create UNet, DiceLoss and Adam optimizer\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=num_classes,\n",
    "        channels=(48, 64, third_channel, fourth_channel),\n",
    "        strides=(2, 2, last_stride),\n",
    "        num_res_units=num_res_units,\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    model.to('cuda')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    autocast_dtype = torch.float16\n",
    "\n",
    "    best_val_score = 0\n",
    "\n",
    "    max_epochs = 15\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=autocast_dtype):\n",
    "                outputs = model(inputs)\n",
    "                loss = tversky_loss_function(outputs, labels)\n",
    "                #fbeta_loss = fbeta_loss_function(outputs, labels)\n",
    "                #tversky_ce_loss = tversky_ce_loss_function(outputs, labels)\n",
    "                #total_loss = fbeta_loss * 0.7 + tversky_ce_loss * 0.3\n",
    "\n",
    "            scaler.scale(loss).backward()  \n",
    "            scaler.step(optimizer)  \n",
    "            scaler.update() \n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            #print(f\"batch {step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
    "            \n",
    "        epoch_loss /= step\n",
    "        #epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        #mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch+1)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=autocast_dtype):\n",
    "                \n",
    "                solution_dfs_list = []\n",
    "                submission_dfs_list = []\n",
    "                exp_names_list = []\n",
    "    \n",
    "                for i in range(len(val_files_copy)):\n",
    "                    val_data = val_files_copy[i]\n",
    "\n",
    "                    exp_name = val_data['label_df']['experiment'][0]\n",
    "                    if exp_name in exp_names_list:\n",
    "                        continue\n",
    "                    else:\n",
    "                        exp_names_list.append(exp_name)\n",
    "        \n",
    "                    tomo = val_data['image']\n",
    "                    tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], 96)\n",
    "                    tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "                    tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0, progress=False)\n",
    "                    pred_masks = []\n",
    "                    for i in range(len(tomo_ds)):\n",
    "                        input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
    "                        max_classes = ensemble_prediction_tta(model, input_tensor, threshold=0.05)\n",
    "                        pred_masks.append(max_classes.cpu().numpy())\n",
    "        \n",
    "                    reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
    "                    location = {}\n",
    "                    for c in classes:\n",
    "                        cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "                        stats = cc3d.statistics(cc)\n",
    "                        zyx = stats['centroids'][1:]\n",
    "                        zyx_large = zyx[stats['voxel_counts'][1:] > 255]\n",
    "                        xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                        location[id_to_name[c]] = xyz\n",
    "                    df = dict_to_df(location, val_data['label_df']['experiment'][0])\n",
    "        \n",
    "                    solution_df = val_data['label_df']\n",
    "    \n",
    "                    solution_dfs_list.append(solution_df)\n",
    "                    submission_dfs_list.append(df)\n",
    "\n",
    "                solution_concat_df = pd.concat(solution_dfs_list, ignore_index=True).reset_index(drop=True).reset_index().rename(columns={'index':'id'})[['id', 'experiment', 'particle_type', 'x', 'y', 'z']]\n",
    "                submission_concat_df = pd.concat(submission_dfs_list, ignore_index=True).reset_index(drop=True).reset_index().rename(columns={'index':'id'})\n",
    "\n",
    "                \n",
    "        \n",
    "                val_fbeta_score = score(solution_concat_df, submission_concat_df, 'id', 0.1*0.5, 4)\n",
    "    \n",
    "                print(f'Epoch {epoch+1} validation F beta score: ', val_fbeta_score)\n",
    "\n",
    "                if val_fbeta_score > best_val_score:\n",
    "                    best_val_score = val_fbeta_score\n",
    "\n",
    "    return best_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_number = '20250130_03'\n",
    "optuna_n_trials = 50\n",
    "\n",
    "with tqdm(total=optuna_n_trials, desc=\"Optimizing\", unit=\"trial\") as pbar:\n",
    "    \n",
    "    # Define a callback function to update the progress bar\n",
    "    def progress_bar_callback(study, trial):\n",
    "        pbar.update(1)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optunahub.load_module(\"samplers/auto_sampler\").AutoSampler(),\n",
    "        storage=\"sqlite:////home/max1024/Python Notebooks/czii/optuna_study/db.sqlite3\",\n",
    "        study_name=f\"czii_3D_UNet_param_tune_{notebook_number}\"\n",
    "    )\n",
    "    study.optimize(objective, n_trials=optuna_n_trials, callbacks=[progress_bar_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://www.kaggle.com/code/hideyukizushi/czii-yolo11-unet3d-monai-lb-707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10033515,
     "sourceId": 84969,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
